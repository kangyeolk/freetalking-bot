"""
Vocabulary analyzer using OpenAI API for Japanese conversation learning
"""

import openai
import json
import streamlit as st
from typing import List, Dict, Optional
from datetime import datetime
import re

class VocabularyAnalyzer:
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
        self.session_vocabulary = []
        self.analyzed_texts = set()  # Track already analyzed texts
        
    def analyze_conversation(self, text: str, role: str = 'assistant') -> List[Dict]:
        """Analyze Japanese text and extract vocabulary with LLM"""
        
        # Skip if already analyzed or too short
        if text in self.analyzed_texts or len(text) < 3:
            return []
        
        # Only analyze assistant messages (Japanese)
        if role != 'assistant':
            return []
            
        self.analyzed_texts.add(text)
        
        prompt = f"""
        ë‹¤ìŒ ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ì—ì„œ í•™ìŠµì— ìœ ìš©í•œ ë‹¨ì–´ì™€ í‘œí˜„ì„ ì¶”ì¶œí•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”.
        ì¤‘ìš”í•œ ë‹¨ì–´, ë¬¸ë²• í¬ì¸íŠ¸, ê´€ìš©êµ¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„ íƒí•´ì£¼ì„¸ìš”.
        
        í…ìŠ¤íŠ¸: {text}
        
        ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "vocabulary": [
                {{
                    "word": "ì›í˜• ë˜ëŠ” í‘œë©´í˜•",
                    "reading": "ížˆë¼ê°€ë‚˜ ì½ê¸°",
                    "meaning_kr": "í•œêµ­ì–´ ëœ»",
                    "meaning_en": "ì˜ì–´ ëœ» (ì˜µì…˜)",
                    "pos": "í’ˆì‚¬ (ëª…ì‚¬/ë™ì‚¬/í˜•ìš©ì‚¬ ë“±)",
                    "level": "JLPT ë ˆë²¨ (N5-N1)",
                    "example": "ì˜ˆë¬¸ (ìžˆìœ¼ë©´)",
                    "notes": "ë¬¸ë²• ì„¤ëª…ì´ë‚˜ ì‚¬ìš©ë²• íŒ (ì˜µì…˜)"
                }}
            ]
        }}
        
        ì£¼ì˜ì‚¬í•­:
        - ë„ˆë¬´ ê¸°ì´ˆì ì¸ ì¡°ì‚¬ë‚˜ ëŒ€ëª…ì‚¬ëŠ” ì œì™¸
        - í•™ìŠµ ê°€ì¹˜ê°€ ìžˆëŠ” ë‹¨ì–´ ì¤‘ì‹¬ìœ¼ë¡œ 5-10ê°œ ì„ íƒ
        - ë¬¸ë§¥ì—ì„œì˜ ì‹¤ì œ ì˜ë¯¸ë¥¼ ë°˜ì˜
        - JLPT ë ˆë²¨ì„ ì •í™•ížˆ íŒë‹¨
        """
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-5-nano",
                messages=[
                    {"role": "system", "content": "You are a Japanese language teacher helping Korean students learn vocabulary."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            vocabulary = result.get('vocabulary', [])
            
            # Add timestamp and source text
            for word in vocabulary:
                word['source_text'] = text[:50] + "..." if len(text) > 50 else text
                word['timestamp'] = datetime.now().isoformat()
                
                # Check if word already exists in session
                exists = any(
                    w.get('word') == word['word'] 
                    for w in self.session_vocabulary
                )
                if not exists:
                    self.session_vocabulary.append(word)
            
            return vocabulary
            
        except Exception as e:
            st.error(f"Vocabulary analysis error: {e}")
            return []
    
    def get_session_vocabulary(self) -> List[Dict]:
        """Get all vocabulary from current session"""
        return self.session_vocabulary
    
    def get_vocabulary_by_level(self) -> Dict[str, List[Dict]]:
        """Group vocabulary by JLPT level"""
        grouped = {
            'N5': [],
            'N4': [],
            'N3': [],
            'N2': [],
            'N1': []
        }
        
        for word in self.session_vocabulary:
            level = word.get('level', 'N3')
            if level in grouped:
                grouped[level].append(word)
        
        return grouped
    
    def export_vocabulary(self) -> str:
        """Export vocabulary as formatted text"""
        output = "ðŸ“š Today's Vocabulary List\n"
        output += "=" * 40 + "\n\n"
        
        grouped = self.get_vocabulary_by_level()
        
        for level in ['N5', 'N4', 'N3', 'N2', 'N1']:
            if grouped[level]:
                output += f"ðŸ“Š {level} Level\n"
                output += "-" * 20 + "\n"
                
                for word in grouped[level]:
                    output += f"â€¢ {word['word']} ({word.get('reading', '')})\n"
                    output += f"  â†’ {word.get('meaning_kr', '')}\n"
                    if word.get('example'):
                        output += f"  ä¾‹: {word['example']}\n"
                    output += "\n"
        
        return output
    
    def clear_session(self):
        """Clear session vocabulary"""
        self.session_vocabulary.clear()
        self.analyzed_texts.clear()
    
    def save_to_file(self, filename: str = "vocabulary.json"):
        """Save vocabulary to file"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.session_vocabulary, f, ensure_ascii=False, indent=2)
    
    def load_from_file(self, filename: str = "vocabulary.json"):
        """Load vocabulary from file"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                self.session_vocabulary = json.load(f)
        except FileNotFoundError:
            pass